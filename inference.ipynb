{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Install necessary packages before running the code"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd11a09c3085875"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9658c750d8ad8f1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tarudesu/ViHateT5-base-HSD\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"tarudesu/ViHateT5-base-HSD\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "50c95ba073d5ba11"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Inference function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccd994540b4d44fc"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def generate_output(input_text, prefix):\n",
    "    # Add prefix\n",
    "    prefixed_input_text = prefix + ': ' + input_text\n",
    "\n",
    "    # Tokenize input text\n",
    "    input_ids = tokenizer.encode(prefixed_input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output\n",
    "    output_ids = model.generate(input_ids, max_length=256)\n",
    "\n",
    "    # Decode the generated output\n",
    "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return output_text\n",
    "\n",
    "sample = 'Tôi ghét bạn vl luôn!'\n",
    "prefix = 'hate-spans-detection'\n",
    "\n",
    "result = generate_output(sample, prefix)\n",
    "result"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "16c9e275ae7bc62d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find index of hate spans"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5788f2dde3827bcd"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "def connect_values(input_list, input_text): # For [hate]abc[hate] [hate]xyz[hate] in the sample 'abc xyz' => Merge the index and insert the idx of the space between two consecutive words\n",
    "\n",
    "    input_list = ast.literal_eval(input_list) # '[]' => []\n",
    "\n",
    "    modified_list = []\n",
    "    temp_list = []\n",
    "\n",
    "    for i in range(len(input_list) - 1):\n",
    "        modified_list.append(input_list[i])\n",
    "\n",
    "        if input_list[i + 1] - input_list[i] == 2:\n",
    "            difference = input_list[i + 1] - input_list[i]\n",
    "            temp_list.extend(input_list[i] + j for j in range(1, difference))\n",
    "\n",
    "    # Check each value in temp_list, if input_text[temp_list[i]] is not a space, remove temp_list[i]\n",
    "    for idx in temp_list:\n",
    "        if input_text[idx] != \" \":\n",
    "            temp_list.remove(idx)\n",
    "\n",
    "    modified_list.extend(temp_list)\n",
    "    modified_list.append(input_list[-1])\n",
    "\n",
    "    return sorted(modified_list)\n",
    "\n",
    "\n",
    "def find_and_extract_substrings(original_str, input_str):\n",
    "\n",
    "    start_tag = '[hate]'\n",
    "    end_tag = '[hate]'\n",
    "\n",
    "    input_str = unicodedata.normalize('NFC', input_str.lower())\n",
    "    original_str = unicodedata.normalize('NFC', original_str.lower())\n",
    "\n",
    "    # Extract substrings\n",
    "    substrings = []\n",
    "    start_index = input_str.find(start_tag)\n",
    "    while start_index != -1:\n",
    "        end_index = input_str.find(end_tag, start_index + len(start_tag))\n",
    "        if end_index != -1:\n",
    "            substrings.append(input_str[start_index + len(start_tag):end_index])\n",
    "            start_index = input_str.find(start_tag, end_index + len(end_tag))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if not substrings:\n",
    "        return ''\n",
    "\n",
    "    # Find indices in the original string and merge into one list\n",
    "    indices_list = []\n",
    "    for substring in substrings:\n",
    "        start_index = original_str.find(substring)\n",
    "        while start_index != -1:\n",
    "            indices_list.extend(list(range(start_index, start_index + len(substring))))\n",
    "            start_index = original_str.find(substring, start_index + 1)\n",
    "\n",
    "    deduplicated_sorted_indices_list = sorted(set(indices_list))\n",
    "\n",
    "    deduplicated_sorted_indices_list = connect_values(str(deduplicated_sorted_indices_list), original_str)\n",
    "\n",
    "    return str(deduplicated_sorted_indices_list)\n",
    "\n",
    "result_indices_str = find_and_extract_substrings(sample, result)\n",
    "result_indices_str"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "672b1e50274daf34"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
